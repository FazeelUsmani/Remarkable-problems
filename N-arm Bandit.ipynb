{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epsilon:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "n_bandit=2000 # number of bandit problems\n",
    "k=10 # number of arms in each bandit problem\n",
    "n_pulls=1000 # number of times to pull each arm\n",
    "\n",
    "q_true=np.random.normal(0,1,(n_bandit,k)) # generating the true means q*(a) for each arm for all bandits\n",
    "true_opt_arms=np.argmax(q_true,1) # the true optimal arms in each bandit\n",
    "# each row represents a bandit problem\n",
    "\n",
    "epsilon=[0,0.01,0.1,0.2,1] # epsilon in epsilon-greedy method\n",
    "col=['r','g','k','b','y']\n",
    "#fig=plt.figure()\n",
    "fig1=plt.figure().add_subplot(111)\n",
    "fig2=plt.figure().add_subplot(111)\n",
    "\n",
    "for eps in range(len(epsilon)) : \n",
    "    print('Current epsilon: ',eps)\n",
    "    Q=np.zeros((n_bandit,k)) # reward estimated\n",
    "    N=np.ones((n_bandit,k)) # number of times each arm was pulled # each arm is pulled atleast once\n",
    "    # Pull all arms once\n",
    "    Qi=np.random.normal(q_true,1) # initial pulling of all arms\n",
    "\n",
    "    R_eps=[]\n",
    "    R_eps.append(0)\n",
    "    R_eps.append(np.mean(Qi))\t\n",
    "    R_eps_opt=[]\n",
    "\n",
    "    for pull in range(2,n_pulls+1) :  \n",
    "        R_pull=[] # all rewards in this pull/time-step\n",
    "        opt_arm_pull=0 # number of pulss of best arm in this time step\n",
    "        for i in range(n_bandit) : \t\n",
    "\n",
    "            if random.random()<epsilon[eps] : \n",
    "                j=np.random.randint(k)\n",
    "            else : \n",
    "                j=np.argmax(Q[i])\n",
    "\n",
    "            if j==true_opt_arms[i] : # To calculate % optimal action\n",
    "                opt_arm_pull=opt_arm_pull+1\n",
    "\n",
    "            temp_R=np.random.normal(q_true[i][j],1)\n",
    "            R_pull.append(temp_R)\n",
    "            N[i][j]=N[i][j]+1\n",
    "            Q[i][j]=Q[i][j]+(temp_R-Q[i][j])/N[i][j]\n",
    "\n",
    "        avg_R_pull=np.mean(R_pull)\n",
    "        R_eps.append(avg_R_pull)\n",
    "        R_eps_opt.append(float(opt_arm_pull)*100/2000)\n",
    "    fig1plot(range(0,n_pulls+1),R_eps,col[eps])\n",
    "    fig2.plot(range(2,n_pulls+1),R_eps_opt,col[eps])\n",
    "\n",
    "plt.rc('text',usetex=True)\n",
    "#plt.ylim(0.5,1.5)\n",
    "fig1.title.set_text(r'$\\epsilon$-greedy : Average Reward Vs Steps for 10 arms')\n",
    "fig1.set_ylabel('Average Reward')\n",
    "fig1.set_xlabel('Steps')\n",
    "fig1.legend((r\"$\\epsilon=$\"+str(epsilon[0]),r\"$\\epsilon=$\"+str(epsilon[1]),r\"$\\epsilon=$\"+str(epsilon[2]),r\"$\\epsilon=$\"+str(epsilon[3]),r\"$\\epsilon=$\"+str(epsilon[4])),loc='best')\n",
    "fig2.title.set_text(r'$\\epsilon$-greedy : $\\%$ Optimal Action Vs Steps for 10 arms')\n",
    "fig2.set_ylabel(r'$\\%$ Optimal Action')\n",
    "fig2.set_xlabel('Steps')\n",
    "fig2.set_ylim(0,100)\n",
    "fig2.legend((r\"$\\epsilon=$\"+str(epsilon[0]),r\"$\\epsilon=$\"+str(epsilon[1]),r\"$\\epsilon=$\"+str(epsilon[2]),r\"$\\epsilon=$\"+str(epsilon[3]),r\"$\\epsilon=$\"+str(epsilon[4])),loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
